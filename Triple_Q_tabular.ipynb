{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Triple-Q",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8Vjyx07xA7w"
      },
      "source": [
        "import numpy as np\n",
        "import operator\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUMsVqYFubJw"
      },
      "source": [
        "# Set random seed to reproduce the grid world map\n",
        "np.random.seed(1234)\n",
        "grids = 20\n",
        "data = np.zeros((grids,grids))\n",
        "data[:,0] = 1\n",
        "data[:,grids-1] = 1\n",
        "data[0,:] = 1\n",
        "data[grids-1,:] = 1\n",
        "\n",
        "ratio = 0.40\n",
        "for i in range(2,grids-2,1):\n",
        "  for j in range(1,grids-1,1):\n",
        "    if np.random.rand() <= ratio:\n",
        "      data[i,j] = 2"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "id": "JmkZTvZvw-_4",
        "outputId": "10c43fcc-27dd-4c36-91ac-68b13df42881"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import colors\n",
        "\n",
        "startx,starty = grids-2,grids-2\n",
        "data[startx,starty] = 3 #starting position\n",
        "goal_y = int(np.random.rand() * (grids-3)) \n",
        "data[1,goal_y+1] = 4    #destination\n",
        "'''\n",
        "#0:normal 1:wall 2:obstacle 3:starting position 4:destination\n",
        "'''\n",
        "cmap = colors.ListedColormap(['black','silver','orange','red','green'])\n",
        "fig,ax = plt.subplots(1,figsize=(8,8))\n",
        "plt.pcolor(data[::-1],cmap=cmap, linewidths=1)\n",
        "plt.xticks(np.arange(0.5,grids+0.5,step=1))\n",
        "plt.yticks(np.arange(0.5,grids+0.5,step=1))\n",
        "xlabels = ['19','18','17','16','15','14','13','12','11','10','9','8','7','6','5','4','3','2','1','0']\n",
        "ylabels = ['19','18','17','16','15','14','13','12','11','10','9','8','7','6','5','4','3','2','1','0']\n",
        "#ylabels = ['4','3','2','1','0']\n",
        "ax.set_yticklabels(ylabels)\n",
        "ax.set_xticklabels(xlabels[::-1])\n",
        "ax.xaxis.tick_top()\n",
        "plt.show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzaOjyU3ySsJ"
      },
      "source": [
        "actions_space = ['U','D','L','R'] # There are four possible actions\n",
        "from numpy import linalg as LA\n",
        "        \n",
        "def moving_average(x, w):\n",
        "    return np.convolve(x, np.ones(w), 'valid') / w\n",
        "\n",
        "class  GridWorld():\n",
        "    def __init__(self):\n",
        "        self.current_location = [startx,starty]\n",
        "        self.max_distance = LA.norm(np.array([18,1]) - np.array([1,goal_y+1]))\n",
        "        \n",
        "    def reset_location(self):\n",
        "        self.current_location = [startx,starty]\n",
        "        \n",
        "    def get_reward(self, location):\n",
        "        \"\"\"Returns the reward for an input position\"\"\"\n",
        "        norm_r = LA.norm(np.array(location) - np.array([1,goal_y+1]))\n",
        "        r = 100 if data[location[0],location[1]] == 4 else (self.max_distance - norm_r)\n",
        "        return r\n",
        "\n",
        "    def check_border(self,location):\n",
        "        if location[0] <= 0 or location[0] >= grids-1 :\n",
        "            return 1\n",
        "        elif location[1] <= 0 or location[1] >= grids-1:\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "        \n",
        "    def make_step(self, action):\n",
        "        # Store previous location\n",
        "        last_location = self.current_location\n",
        "        # UP\n",
        "        if action == 'U':\n",
        "            border_ = self.check_border([last_location[0]-1, last_location[1]])\n",
        "            if border_ == 1:\n",
        "                reward = self.get_reward(last_location)\n",
        "            else:\n",
        "                reward = self.get_reward([last_location[0] - 1, last_location[1]]) \n",
        "                self.current_location = [last_location[0] - 1, last_location[1]]             \n",
        "        # DOWN\n",
        "        elif action == 'D':\n",
        "            border_ = self.check_border([last_location[0] + 1, last_location[1]])\n",
        "            if border_ == 1:\n",
        "                reward = self.get_reward(last_location)\n",
        "            else:\n",
        "                reward = self.get_reward([last_location[0] + 1, last_location[1]]) \n",
        "                self.current_location = [last_location[0] + 1, last_location[1]]\n",
        "\n",
        "        elif action == 'L':\n",
        "            border_ = self.check_border([last_location[0], last_location[1] - 1])\n",
        "            if border_ == 1:\n",
        "                reward = self.get_reward(last_location)\n",
        "            else:\n",
        "                reward = self.get_reward([last_location[0], last_location[1] - 1]) \n",
        "                self.current_location = [last_location[0], last_location[1] - 1]\n",
        "                \n",
        "        elif action == 'R':\n",
        "            border_ = self.check_border([last_location[0], last_location[1] + 1])\n",
        "            if border_ == 1:\n",
        "                reward = self.get_reward(last_location)\n",
        "            else:\n",
        "                reward = self.get_reward([last_location[0], last_location[1] + 1]) \n",
        "                self.current_location = [last_location[0], last_location[1] + 1]\n",
        "        d = 0\n",
        "        cost = 1 if data[self.current_location[0],self.current_location[1]] == 2 else 0\n",
        "                         \n",
        "        return reward, cost, d\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ng_9Lyw31AA_"
      },
      "source": [
        "import math\n",
        "\n",
        "#CMDP Agent\n",
        "class Q_Agent_cmdp():\n",
        "    # Initilaze\n",
        "    def __init__(self,env,use_lr=True,use_ucb=True,lr=0.1,use_greddy=True,use_epsilon=False,show_z=True,decay_step=500,total_steps=40000):\n",
        "        self.env = env\n",
        "        self.actions_space = ['U','D','L','R']\n",
        "        self.H = 25\n",
        "        self.S = (grids-2)*(grids-2)\n",
        "        self.A = 4\n",
        "        self.K = total_steps\n",
        "        self.decay_step = decay_step\n",
        "        self.delta = 0.03\n",
        "        if use_greddy:\n",
        "          self.epsilon = 1\n",
        "        else:\n",
        "          self.epsilon = 0\n",
        "        \n",
        "        self.episode_reward = []\n",
        "        self.Z = 0\n",
        "        self.qtable = np.zeros((self.H,grids,grids,4)) + 800\n",
        "        self.ctable = np.zeros((self.H,grids,grids,4)) + 25\n",
        "        self.vtable = np.zeros((self.H,grids,grids)) + 800\n",
        "        self.wtable = np.zeros((self.H,grids,grids)) + 25\n",
        "        self.decay_step = decay_step\n",
        "        self.vtable[self.H-1,:,:] = 0\n",
        "        self.wtable[self.H-1,:,:] = 0\n",
        "        self.numbers = np.zeros((self.H,grids,grids,4))\n",
        "        self.frame_length = int(self.K ** 0.6)\n",
        "        self.rho = 20\n",
        "        self.iota =  math.log(math.sqrt(2*self.S*self.A*self.H)*self.K)\n",
        "        self.epsilon_z = 0.5\n",
        "        self.epsilon = 1\n",
        "        self.eta = 0.0001 * self.K ** 0.2 * math.sqrt(self.A*self.S*self.H**4)\n",
        "        print(\"eta is {}\".format(self.eta))\n",
        "        self.qhi = self.K ** 0.2\n",
        "        self.episode_reward = [] \n",
        "        self.lr = lr\n",
        "        self.use_lr = use_lr\n",
        "        self.use_ucb = use_ucb\n",
        "        self.show_z=show_z\n",
        "        self.use_epsilon = use_epsilon\n",
        "        \n",
        "    def choose_action(self, h, state):\n",
        "        if self.use_epsilon:\n",
        "            if np.random.rand() < self.epsilon:\n",
        "                a_ = np.random.choice(4,1)\n",
        "                action = self.actions_space[a_[0]]\n",
        "                return action\n",
        "        if np.random.rand() <= self.delta:\n",
        "            a_ = np.random.choice(4,1)\n",
        "            action = self.actions_space[a_[0]]\n",
        "            return action\n",
        "        actions = self.qtable[h,state[0], state[1],:] + (self.Z / self.eta) * self.ctable[h,state[0], state[1],:]\n",
        "        action_index = np.random.choice(np.where(actions == actions.max())[0])\n",
        "        return self.actions_space[int(action_index)]\n",
        "\n",
        "    def use_action(self, h, state):\n",
        "        if np.random.rand() <= self.delta:\n",
        "            a_ = np.random.choice(4,1)\n",
        "            action = self.actions_space[a_[0]]\n",
        "            return action\n",
        "        actions = self.qtable[h,state[0], state[1],:] + (self.Z / self.eta) * self.ctable[h,state[0], state[1],:]\n",
        "        action_index = np.random.choice(np.where(actions == actions.max())[0])\n",
        "        return self.actions_space[int(action_index)]\n",
        "\n",
        "    \n",
        "    def learn(self, states, actions, rewards, costs, dones):\n",
        "        for i in range(len(states)-1):\n",
        "            a_index = self.actions_space.index(actions[i])\n",
        "            self.vtable[i,states[i][0], states[i][1]] = self.qtable[i,states[i][0], states[i][1],a_index]\n",
        "            self.wtable[i,states[i][0], states[i][1]] = self.ctable[i,states[i][0], states[i][1],a_index]\n",
        "            \n",
        "        for i in range(len(states)-1):\n",
        "            a_index = self.actions_space.index(actions[i])\n",
        "            self.numbers[i, states[i][0],states[i][1],a_index] += 1\n",
        "            t = self.numbers[i,states[i][0],states[i][1],a_index]\n",
        "            alpha = 1 * (self.qhi + 1.) / (self.qhi + t) if self.use_lr else self.lr\n",
        "            b =  0.2 * np.sqrt( np.log(self.K ) / self.numbers[i,states[i][0],states[i][1],a_index]) if self.use_ucb else 0 \n",
        "            if dones[i]:\n",
        "                self.qtable[i,states[i][0], states[i][1], a_index] = (1 - alpha) * self.qtable[i,states[i][0], states[i][1], a_index] + alpha * (rewards[i] + b)\n",
        "                self.ctable[i,states[i][0], states[i][1], a_index] = (1 - alpha) * self.ctable[i,states[i][0], states[i][1], a_index] + alpha * (costs[i]+ 0.01 * b)\n",
        "            else:\n",
        "                self.qtable[i,states[i][0], states[i][1], a_index] = (1 - alpha) * self.qtable[i,states[i][0], states[i][1], a_index] + alpha * (rewards[i] + self.vtable[i+1,states[i+1][0],states[i+1][1]] +  b)\n",
        "                self.ctable[i,states[i][0], states[i][1], a_index] = (1 - alpha) * self.ctable[i,states[i][0], states[i][1], a_index] + alpha * (costs[i] + self.wtable[i+1,states[i+1][0],states[i+1][1]] +  0.01 * b)\n",
        "        \n",
        "    def update_z(self,cbar):\n",
        "        self.Z = max(self.Z + self.rho + self.epsilon_z - cbar / self.frame_length , 0)\n",
        "        \n",
        "    def play(self):\n",
        "        cbar = 0\n",
        "        test_score = 0\n",
        "        total_steps = 0\n",
        "        collision = 0\n",
        "\n",
        "        test_rewards = []\n",
        "        test_costs = []\n",
        "        \n",
        "        for trial in range(self.K): # Run trials\n",
        "            cumulative_reward = 0 # Initialise values of each game\n",
        "            self.env.reset_location()\n",
        "            actions_epi = []\n",
        "            locations = []\n",
        "            wall_time = 0\n",
        "            \n",
        "            if trial % self.frame_length == 0 and trial != 0: #Reset at the beginning of each frame\n",
        "                self.qtable +=  3\n",
        "                self.qtable[(self.qtable > 850) + (self.ctable > 25)] = 800\n",
        "                self.update_z(cbar)\n",
        "                if self.show_z:\n",
        "                  print(\"z at frmae {} is {}\".format((trial / self.frame_length),self.Z))\n",
        "                  print(\"cbar at frame {} is {}\".format((trial / self.frame_length),(cbar / self.frame_length)))\n",
        "                self.numbers = np.zeros((self.H,grids,grids,4))\n",
        "                cbar = 0.\n",
        "                       \n",
        "            states = []\n",
        "            actions = []\n",
        "            rewards = []\n",
        "            costs = []\n",
        "            dones = []\n",
        "            states.append(self.env.current_location)\n",
        "          \n",
        "            for h in range(self.H):\n",
        "                old_state = self.env.current_location\n",
        "                action = self.choose_action(h,old_state)\n",
        "                actions.append(action)\n",
        "                \n",
        "                reward, cost, done = self.env.make_step(action)\n",
        "                cost =  1 - cost\n",
        "                new_state = self.env.current_location \n",
        "                states.append(new_state)\n",
        "                rewards.append(reward)\n",
        "                costs.append(cost)\n",
        "                \n",
        "                if (cost) == 0:\n",
        "                    wall_time += 1\n",
        "                    \n",
        "                total_steps += 1\n",
        "                actions_epi.append(action)          \n",
        "                cumulative_reward += reward\n",
        "                \n",
        "                done = 1 if h == self.H -1 else done\n",
        "                dones.append(done)\n",
        "                   \n",
        "            self.learn(states, actions, rewards, costs, dones)\n",
        "            if trial % self.decay_step == 0:\n",
        "                self.epsilon = self.epsilon * 0.95\n",
        "      \n",
        "            a1 = self.actions_space.index(actions[0])\n",
        "            cbar += self.ctable[0,states[0][0],states[0][1],a1]\n",
        "          \n",
        "            self.episode_reward.append(cumulative_reward) \n",
        "            if trial % 10000 == 0:\n",
        "                print (f\"mean reward: {np.mean(self.episode_reward)}\")\n",
        "\n",
        "            test_score,test_cost, finish_ = self.test()\n",
        "            test_rewards.append(test_score)\n",
        "            test_costs.append(test_cost)\n",
        "        return test_rewards,test_costs\n",
        "\n",
        "\n",
        "    def test(self):\n",
        "        succ = 0\n",
        "        test_cost = 0\n",
        "        for i in range(1):\n",
        "            self.env.reset_location()\n",
        "            actions_epi = []\n",
        "            locations = []    \n",
        "            finish = 0\n",
        "            locations.append(self.env.current_location)\n",
        "            for h in range(self.H):\n",
        "                old_state = self.env.current_location\n",
        "                action = self.use_action(h,old_state)\n",
        "                reward, cost, done = self.env.make_step(action) \n",
        "                succ += reward\n",
        "                test_cost += cost\n",
        "                actions_epi.append(action)\n",
        "                locations.append(self.env.current_location)\n",
        "\n",
        "        return succ, test_cost, finish"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRWcfm3x1Mbx"
      },
      "source": [
        "cmdp_c = []\n",
        "cmdp_r = []\n",
        "for i in range(1):\n",
        "    np.random.seed(1234+1234*i)\n",
        "    env = GridWorld()\n",
        "    agent = Q_Agent_cmdp(env,show_z = False, use_ucb=True,use_lr=True, use_epsilon=True, decay_step=20,total_steps=80000)\n",
        "    r_,c_ = agent.play()\n",
        "    cmdp_c.append(c_)\n",
        "    cmdp_r.append(r_)\n",
        "\n",
        "cmdp_r = np.mean(cmdp_r,axis=0)\n",
        "cmdp_c = np.mean(cmdp_c,axis=0)\n",
        "\n",
        "#np.save(\"./avg_r_tripleq.npy\", cmdp_r2) #Save testing rewards and costs\n",
        "#np.save(\"./avg_c_tripleq.npy\", cmdp_c2)\n",
        "\n",
        "cmdp_r_mvg = moving_average(cmdp_r,1000) #Moving average for better visualization \n",
        "cmdp_c_mvg = moving_average(cmdp_c,1000)\n",
        "a1 = plt.subplot(1,2,1)\n",
        "a1.plot(cmdp_r_mvg)\n",
        "a1 = plt.subplot(1,2,2)\n",
        "a1.plot(cmdp_c_mvg)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}